{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c63bfc8-4d3f-4b0e-af0c-2895dcf057e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Core Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "# --- Machine Learning ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# --- Visualization ---\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_DIR = 'datasets'\n",
    "OUTPUT_DIR = 'models'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20fcaffd-4074-4ff6-9cba-02da60c20075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Step 1/4] Starting Simplified Preprocessing with Data Augmentation ---\n",
      "--> Processing dataset1.csv...\n",
      "--> Processing dataset10.csv...\n",
      "--> Processing dataset2.csv...\n",
      "--> Processing dataset3.csv...\n",
      "    -> Augmenting 'sql_injection' with 947 new samples.\n",
      "--> Processing dataset4.csv...\n",
      "--> Processing dataset5.csv...\n",
      "--> Processing dataset6.csv...\n",
      "--> Processing dataset7.csv...\n",
      "    -> Augmenting 'sql_injection' with 966 new samples.\n",
      "--> Processing dataset8.csv...\n",
      "--> Processing dataset9.csv...\n",
      "\n",
      "âœ… All raw datasets have been processed, augmented, and balanced in memory.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- [Step 1/4] Starting Simplified Preprocessing with Data Augmentation ---\")\n",
    "\n",
    "files_to_process = glob.glob(os.path.join(DATASET_DIR, \"dataset*.csv\"))\n",
    "all_processed_dfs = []\n",
    "\n",
    "label_map = {\n",
    "    'DoS attacks-Hulk': 'dos_ddos', 'DoS attacks-GoldenEye': 'dos_ddos',\n",
    "    'DoS attacks-Slowloris': 'dos_ddos', 'DoS attacks-SlowHTTPTest': 'dos_ddos',\n",
    "    'DDOS attack-HOIC': 'dos_ddos', 'DDoS attacks-LOIC-HTTP': 'dos_ddos',\n",
    "    'DDOS attack-LOIC-UDP': 'dos_ddos', 'FTP-BruteForce': 'bruteforce',\n",
    "    'SSH-BruteForce': 'bruteforce', 'Brute Force -Web': 'bruteforce',\n",
    "    'Brute Force -XSS': 'bruteforce', 'Bot': 'bot',\n",
    "    'Infilteration': 'infiltration', 'SQL Injection': 'sql_injection', 'Benign': 'benign'\n",
    "}\n",
    "\n",
    "# ðŸ’¡ NEW: Define how many samples we want for our minority classes\n",
    "augmentation_targets = {\n",
    "    'infiltration': 10000,\n",
    "    'sql_injection': 1000\n",
    "}\n",
    "\n",
    "for filepath in files_to_process:\n",
    "    filename = os.path.basename(filepath)\n",
    "    print(f\"--> Processing {filename}...\")\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    \n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    if 'Label' in df.columns and 'Label' in df['Label'].unique():\n",
    "        df = df[df['Label'] != 'Label'].copy()\n",
    "        \n",
    "    new_cols = [re.sub(r'[^a-zA-Z0-9]+', '_', col).lower().strip('_') for col in df.columns]\n",
    "    df.columns = new_cols\n",
    "    \n",
    "    if 'label' in df.columns:\n",
    "        df['label'] = df['label'].map(label_map).fillna(df['label'])\n",
    "\n",
    "        # --- ðŸ’¡ NEW: DATA AUGMENTATION / OVERSAMPLING LOGIC ---\n",
    "        new_samples = []\n",
    "        for label, target_count in augmentation_targets.items():\n",
    "            if label in df['label'].values:\n",
    "                label_df = df[df['label'] == label]\n",
    "                current_count = len(label_df)\n",
    "                \n",
    "                if current_count > 0 and current_count < target_count:\n",
    "                    # Calculate how many new samples to add\n",
    "                    num_to_add = target_count - current_count\n",
    "                    # Create new samples by duplicating existing ones\n",
    "                    augmented_samples = label_df.sample(n=num_to_add, replace=True, random_state=42)\n",
    "                    new_samples.append(augmented_samples)\n",
    "                    print(f\"    -> Augmenting '{label}' with {num_to_add} new samples.\")\n",
    "        \n",
    "        # Add the new samples back to the dataframe\n",
    "        if new_samples:\n",
    "            df = pd.concat([df] + new_samples, ignore_index=True)\n",
    "        # --- END OF NEW LOGIC ---\n",
    "\n",
    "        class_counts = df['label'].value_counts()\n",
    "        if len(class_counts) > 1:\n",
    "            majority_class_name = class_counts.index[0]\n",
    "            target_size = class_counts.iloc[1]\n",
    "            df_majority = df[df['label'] == majority_class_name]\n",
    "            df_others = df[df['label'] != majority_class_name]\n",
    "            df_majority_downsampled = df_majority.sample(n=target_size, random_state=42)\n",
    "            df_balanced = pd.concat([df_majority_downsampled, df_others])\n",
    "            all_processed_dfs.append(df_balanced)\n",
    "        else:\n",
    "            all_processed_dfs.append(df)\n",
    "\n",
    "print(\"\\nâœ… All raw datasets have been processed, augmented, and balanced in memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d360240-9315-4024-9411-63e0cf1ac155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Step 2/4] Merging and Final Bulletproof Cleaning ---\n",
      "-> Combined dataset shape: (4347988, 84)\n",
      "--> Forcing feature columns to numeric type...\n",
      "--> Cleaning and clipping data...\n",
      "\n",
      "âœ… Merging and final cleaning complete. Data is guaranteed to be valid.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- [Step 2/4] Merging and Final Bulletproof Cleaning ---\")\n",
    "\n",
    "# --- Merge all dataframes first ---\n",
    "final_df = pd.concat(all_processed_dfs, ignore_index=True)\n",
    "print(f\"-> Combined dataset shape: {final_df.shape}\")\n",
    "\n",
    "# --- Identify feature and identifier columns ---\n",
    "identifier_cols = ['src_ip', 'src_port', 'dst_ip', 'timestamp', 'flow_id']\n",
    "label_col = 'label'\n",
    "# Get all columns that are features for the model\n",
    "feature_cols = [col for col in final_df.columns if col not in identifier_cols + [label_col]]\n",
    "\n",
    "# --- Force all feature columns to numeric, coercing errors ---\n",
    "print(\"--> Forcing feature columns to numeric type...\")\n",
    "final_df[feature_cols] = final_df[feature_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# --- Replace infinities, fill all NaNs, and clip large values ---\n",
    "print(\"--> Cleaning and clipping data...\")\n",
    "final_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "final_df.fillna(0, inplace=True)\n",
    "finfo = np.finfo(np.float32)\n",
    "final_df[feature_cols] = final_df[feature_cols].clip(finfo.min, finfo.max)\n",
    "\n",
    "print(\"\\nâœ… Merging and final cleaning complete. Data is guaranteed to be valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7613f777-e45f-43a3-8d6f-70d07dbde984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Dataset Class Distribution ---\n",
      "label\n",
      "benign            2079779\n",
      "dos_ddos          1436294\n",
      "bot                286191\n",
      "bruteforce         194201\n",
      "SSH-Bruteforce     187589\n",
      "infiltration       161934\n",
      "sql_injection        2000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Final Dataset Class Distribution ---\")\n",
    "\n",
    "# Use value_counts() on the 'label' column of your final DataFrame\n",
    "label_counts = final_df['label'].value_counts()\n",
    "\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05686d26-0e7e-486a-b245-db5ba3bdd6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Step 3/4] Preparing Data for Modeling ---\n",
      "-> Data split into training ((3478390, 78)) and testing ((869598, 78)) sets.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- [Step 3/4] Preparing Data for Modeling ---\")\n",
    "\n",
    "# --- Select only the feature columns for X ---\n",
    "feature_cols = [col for col in final_df.columns if col not in ['label'] + identifier_cols]\n",
    "X = final_df[feature_cols]\n",
    "y = final_df['label']\n",
    "\n",
    "# Encode labels and split data\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"-> Data split into training ({X_train.shape}) and testing ({X_test.shape}) sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b41e4b53-5028-4ec7-b200-f4b4aa2ec234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Step 4/4] Training and Evaluating Models ---\n",
      "\n",
      "=============== Training Random Forest ===============\n",
      "-> Training completed in 3.25 minutes.\n",
      "\n",
      "--- Random Forest Classification Report ---\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "SSH-Bruteforce       1.00      1.00      1.00     37518\n",
      "        benign       0.95      0.98      0.96    415956\n",
      "           bot       1.00      1.00      1.00     57238\n",
      "    bruteforce       0.76      0.91      0.83     38840\n",
      "      dos_ddos       0.99      0.96      0.97    287259\n",
      "  infiltration       0.50      0.31      0.39     32387\n",
      " sql_injection       0.99      1.00      0.99       400\n",
      "\n",
      "      accuracy                           0.95    869598\n",
      "     macro avg       0.88      0.88      0.88    869598\n",
      "  weighted avg       0.94      0.95      0.94    869598\n",
      "\n",
      "-> Random Forest Accuracy: 94.60%\n",
      "\n",
      "=============== Training XGBoost ===============\n",
      "-> Training completed in 1.68 minutes.\n",
      "\n",
      "--- XGBoost Classification Report ---\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "SSH-Bruteforce       1.00      1.00      1.00     37518\n",
      "        benign       0.95      0.99      0.97    415956\n",
      "           bot       1.00      1.00      1.00     57238\n",
      "    bruteforce       0.76      0.91      0.83     38840\n",
      "      dos_ddos       0.99      0.96      0.97    287259\n",
      "  infiltration       0.74      0.28      0.40     32387\n",
      " sql_injection       0.98      1.00      0.99       400\n",
      "\n",
      "      accuracy                           0.95    869598\n",
      "     macro avg       0.92      0.88      0.88    869598\n",
      "  weighted avg       0.95      0.95      0.95    869598\n",
      "\n",
      "-> XGBoost Accuracy: 95.28%\n",
      "\n",
      "=============== Training LightGBM ===============\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13872\n",
      "[LightGBM] [Info] Number of data points in the train set: 3478390, number of used features: 70\n",
      "[LightGBM] [Info] Start training from score -3.143216\n",
      "[LightGBM] [Info] Start training from score -0.737452\n",
      "[LightGBM] [Info] Start training from score -2.720808\n",
      "[LightGBM] [Info] Start training from score -3.108573\n",
      "[LightGBM] [Info] Start training from score -1.107647\n",
      "[LightGBM] [Info] Start training from score -3.290281\n",
      "[LightGBM] [Info] Start training from score -7.684321\n",
      "-> Training completed in 1.25 minutes.\n",
      "\n",
      "--- LightGBM Classification Report ---\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "SSH-Bruteforce       0.75      0.98      0.85     37518\n",
      "        benign       0.92      0.95      0.94    415956\n",
      "           bot       1.00      0.98      0.99     57238\n",
      "    bruteforce       0.52      0.97      0.68     38840\n",
      "      dos_ddos       0.99      0.86      0.92    287259\n",
      "  infiltration       0.51      0.18      0.27     32387\n",
      " sql_injection       0.15      0.29      0.20       400\n",
      "\n",
      "      accuracy                           0.90    869598\n",
      "     macro avg       0.69      0.75      0.69    869598\n",
      "  weighted avg       0.91      0.90      0.89    869598\n",
      "\n",
      "-> LightGBM Accuracy: 89.62%\n",
      "\n",
      "âœ… All models trained and evaluated.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- [Step 4/4] Training and Evaluating Models ---\")\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, n_jobs=-1, eval_metric='mlogloss'),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*15} Training {name} {'='*15}\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"-> Training completed in {((end_time - start_time) / 60):.2f} minutes.\")\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions) * 100\n",
    "    \n",
    "    print(f\"\\n--- {name} Classification Report ---\")\n",
    "    print(classification_report(y_test, predictions, target_names=label_encoder.classes_))\n",
    "    print(f\"-> {name} Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\nâœ… All models trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf965a6f-09eb-4747-b26a-1e5393220610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Final Step] Saving Trained Models and Encoder ---\n",
      "-> Random Forest model saved to: models\\cyber_threat_model_randomforest.joblib\n",
      "-> XGBoost model saved to: models\\cyber_threat_model_xgboost.joblib\n",
      "-> LightGBM model saved to: models\\cyber_threat_model_lightgbm.joblib\n",
      "-> Label encoder saved to: models\\label_encoder.joblib\n",
      "\n",
      "âœ… All baseline assets exported successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"--- [Final Step] Saving Trained Models and Encoder ---\")\n",
    "\n",
    "# This assumes the 'models' dictionary from the training cell is available\n",
    "# and OUTPUT_DIR is set to 'models'\n",
    "\n",
    "# Save each trained model from the dictionary\n",
    "for name, model in models.items():\n",
    "    filename = f\"cyber_threat_model_{name.lower().replace(' ', '')}.joblib\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, filename)\n",
    "    joblib.dump(model, output_path)\n",
    "    print(f\"-> {name} model saved to: {output_path}\")\n",
    "\n",
    "# Save the label encoder, which is essential for decoding predictions later\n",
    "encoder_filename = os.path.join(OUTPUT_DIR, 'label_encoder.joblib')\n",
    "joblib.dump(label_encoder, encoder_filename)\n",
    "print(f\"-> Label encoder saved to: {encoder_filename}\")\n",
    "\n",
    "print(\"\\nâœ… All baseline assets exported successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
